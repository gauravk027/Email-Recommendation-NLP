{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d69cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cr245297\\AppData\\Local\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541e6518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cr245297\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cr245297\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cr245297\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('F:\\\\NLP\\\\dis 2022-2023 v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f41457e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79910, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b7cf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b463c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('#', ' number ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    # Remove stop words\n",
    "    new_text = []\n",
    "    \n",
    "    for word in q.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    q = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9d25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subcategory_comments'] = data['subcategory_comments'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24fd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spelling check and fix\n",
    "def correct_spelling_data(text):\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    #Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "         \n",
    "    #join the corrected words back into the data\n",
    "    corrected_text_data = ' '.join(corrected_words)\n",
    "    \n",
    "    return corrected_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a42295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['subcategory_comments'] = data['subcategory_comments'].apply(correct_spelling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772002c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization \n",
    "def lemmatize_data(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_data = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fbf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subcategory_comments'] = data['subcategory_comments'].apply(lemmatize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2849792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76f54c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35703"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subcategory_comments'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5fc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(data_toclean):\n",
    "    words = data_toclean.split()\n",
    "    clean_data = ' '.join(words)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba22a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subcategory_comments'] = data['subcategory_comments'].apply(clean_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7cd0ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35703"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subcategory_comments'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eb40802",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset='subcategory_comments', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c842855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44207, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31d38b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"cleaned_data.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ccbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6091e698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to find documemts with first 5 words similar \n",
    "\n",
    "# similar_rows = []\n",
    "# for i in range(len(data)):\n",
    "#    for j in range(i + 1, len(data)):\n",
    "#        words_i = data.iloc[i,9].split()[:5]\n",
    "#        words_j = data.iloc[j,9].split()[:5]\n",
    "#        if words_i == words_j:\n",
    "#            similar_rows.append((i, j))\n",
    "        \n",
    "        \n",
    "\n",
    "similar_rows = []\n",
    "word_sets = [set(row.split()[:3]) for row in data.iloc[:, 9]]\n",
    "for i, j in itertools.combinations(range(len(data)), 2):\n",
    "   if word_sets[i] == word_sets[j]:\n",
    "       similar_rows.append(data.index[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36e76049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234912\n"
     ]
    }
   ],
   "source": [
    "#print(len(rows_to_remove))\n",
    "print(len(similar_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd841ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete similar rows from the dataset\n",
    "# for i, j in similar_rows:\n",
    "#    del data[j]\n",
    "\n",
    "data.drop(similar_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "521c3618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16751, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b3cb36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Function to find similarity between two strings\n",
    "# def similarity(row1, row2):\n",
    "#    # Handle NaN values\n",
    "#    if type(row1) == float and np.isnan(row1):\n",
    "#        row1 = ''\n",
    "#    if type(row2) == float and np.isnan(row2):\n",
    "#        row2 = ''\n",
    "#    return fuzz.ratio(row1, row2)\n",
    "\n",
    "# # # Function to find similarity between two strings\n",
    "# # def similarity(row1, row2):\n",
    "# #    return fuzz.ratio(row1, row2)\n",
    "# # Threshold for similarity (adjust as needed)\n",
    "# similarity_threshold = 30\n",
    "# # List to store indices of rows to be removed\n",
    "# rows_to_remove = []\n",
    "# # Iterate through rows for comparison\n",
    "# for i in range(len(data)):\n",
    "#    if i not in rows_to_remove:\n",
    "#        for j in range(i + 1, len(data)):\n",
    "#            if j not in rows_to_remove:\n",
    "#                #if similarity(data['subcategory_comments'][i], data['subcategory_comments'][j]) > similarity_threshold:\n",
    "#                if similarity(data.iloc[i,9], data.iloc[j,9]) > similarity_threshold:     \n",
    "#                    rows_to_remove.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6b14ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop rows with high similarity\n",
    "# data_cleaned = data.drop(rows_to_remove, axis='index')\n",
    "\n",
    "indexes_to_keep = set(range(data.shape[0]))- set(similar_rows)\n",
    "data_cleaned = data.take(list(indexes_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77acecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "data_cleaned.to_excel('cleaned_data_new.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9400ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "print(data_cleaned.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a25379f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cr245297\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cr245297\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd42f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_data(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = word_tokenize(text)\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#     lemmatized_data = ' '.join(lemmatized_tokens)\n",
    "#     return lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ce58fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have three vision for India . In 3000 year of our history , people from all over the world have come and invaded u , captured our land , conquered our mind\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I have three visions for India. In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds\"\n",
    "cleaned_text = lemmatize_data(input_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "387d513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57fcb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correct_spelling_data(text):\n",
    "#     spell = SpellChecker()\n",
    "    \n",
    "#     #Split the text into words\n",
    "#     words = text.split()\n",
    "    \n",
    "#     corrected_words = []\n",
    "#     for word in words:\n",
    "#         #Get the corrected version of each word\n",
    "#         corrected_word = spell.correction(word)\n",
    "#         corrected_words.append(corrected_word)\n",
    "        \n",
    "#     #join the corrected words back into the data\n",
    "#     corrected_text_data = ' '.join(corrected_words)\n",
    "    \n",
    "#     return corrected_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c8847a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tis is a sample sentence with spelling mistakes\n"
     ]
    }
   ],
   "source": [
    "input_text = \"tis is a sample sentenc wth speling mistakes\"\n",
    "cleaned_text = correct_spelling_data(input_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d1330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585fb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
